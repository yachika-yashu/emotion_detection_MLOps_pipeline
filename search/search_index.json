{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Emotion Detection \u2013 Reproducible ML Pipeline with DVC This project demonstrates how a notebook-based machine learning experiment can be transformed into a fully reproducible MLOps pipeline using DVC, Git, and a Cookiecutter Data Science (CCDS) structure. The pipeline classifies text into two emotions: Happiness Sadness The real goal is not just prediction, but engineering discipline: reproducibility, traceability, modularity, and experiment control.","title":"Home"},{"location":"#emotion-detection-reproducible-ml-pipeline-with-dvc","text":"This project demonstrates how a notebook-based machine learning experiment can be transformed into a fully reproducible MLOps pipeline using DVC, Git, and a Cookiecutter Data Science (CCDS) structure. The pipeline classifies text into two emotions: Happiness Sadness The real goal is not just prediction, but engineering discipline: reproducibility, traceability, modularity, and experiment control.","title":"Emotion Detection \u2013 Reproducible ML Pipeline with DVC"},{"location":"dvc/","text":"DVC Pipeline & Reproducibility Each pipeline stage is registered using dvc stage add, defining: Dependencies (code and data) Outputs Parameters Once defined, the entire workflow can be reproduced using: dvc repro DVC tracks: Intermediate datasets Trained models Evaluation metrics This guarantees that results are reproducible across machines and time.","title":"DVC & Reproducibility"},{"location":"dvc/#dvc-pipeline-reproducibility","text":"Each pipeline stage is registered using dvc stage add, defining: Dependencies (code and data) Outputs Parameters Once defined, the entire workflow can be reproduced using: dvc repro","title":"DVC Pipeline &amp; Reproducibility"},{"location":"dvc/#dvc-tracks","text":"Intermediate datasets Trained models Evaluation metrics This guarantees that results are reproducible across machines and time.","title":"DVC tracks:"},{"location":"getting-started/","text":"Getting started This is where you describe how to get set up on a clean install, including the commands necessary to get the raw data (using the sync_data_from_s3 command, for example), and then how to make the cleaned, final data sets.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"This is where you describe how to get set up on a clean install, including the commands necessary to get the raw data (using the sync_data_from_s3 command, for example), and then how to make the cleaned, final data sets.","title":"Getting started"},{"location":"parameters/","text":"Parameter Management All tunable values are stored in params.yaml. This allows experiments to be rerun without touching code. Example: data_ingestion: test_size: 0.2 feature_engineering: max_features: 50 model_building: n_estimators: 25 learning_rate: 0.1 Changing any parameter automatically triggers only the affected pipeline stages when running dvc repro.","title":"Parameters"},{"location":"parameters/#parameter-management","text":"All tunable values are stored in params.yaml. This allows experiments to be rerun without touching code. Example: data_ingestion: test_size: 0.2 feature_engineering: max_features: 50 model_building: n_estimators: 25 learning_rate: 0.1 Changing any parameter automatically triggers only the affected pipeline stages when running dvc repro.","title":"Parameter Management"},{"location":"pipeline/","text":"ML Pipeline Stages The pipeline is composed of five deterministic stages, each implemented as an independent script and tracked by DVC. 1. Data Ingestion Loads the raw dataset from a remote source Filters only happiness and sadness labels Converts labels to binary format Performs train/test split Outputs written to data/raw/ 2. Data Preprocessing Normalizes text data Steps include lowercasing, stopword removal, lemmatization, and noise removal Cleaned text stored in data/interim/ 3. Feature Engineering Converts text into numerical features using TF-IDF Feature size controlled through parameters Outpu- ts written to data/processed/ 4. Model Building Trains a Gradient Boosting classifier Hyperparameters are externally configurable Trained model saved as models/model.pkl 5. Model Evaluation Computes Accuracy, Precision, Recall, and AUC Metrics stored in reports/metrics.json","title":"ML Pipeline"},{"location":"pipeline/#ml-pipeline-stages","text":"The pipeline is composed of five deterministic stages, each implemented as an independent script and tracked by DVC.","title":"ML Pipeline Stages"},{"location":"pipeline/#1-data-ingestion","text":"Loads the raw dataset from a remote source Filters only happiness and sadness labels Converts labels to binary format Performs train/test split Outputs written to data/raw/","title":"1. Data Ingestion"},{"location":"pipeline/#2-data-preprocessing","text":"Normalizes text data Steps include lowercasing, stopword removal, lemmatization, and noise removal Cleaned text stored in data/interim/","title":"2. Data Preprocessing"},{"location":"pipeline/#3-feature-engineering","text":"Converts text into numerical features using TF-IDF Feature size controlled through parameters Outpu- ts written to data/processed/","title":"3. Feature Engineering"},{"location":"pipeline/#4-model-building","text":"Trains a Gradient Boosting classifier Hyperparameters are externally configurable Trained model saved as models/model.pkl","title":"4. Model Building"},{"location":"pipeline/#5-model-evaluation","text":"Computes Accuracy, Precision, Recall, and AUC Metrics stored in reports/metrics.json","title":"5. Model Evaluation"},{"location":"project_overview/","text":"Project Overview The project began as a single Jupyter notebook used for experimentation. Once the modeling approach was validated, the notebook was refactored into a production-style pipeline composed of independent Python modules. Key ideas demonstrated: Separation of concerns Script-based execution instead of notebooks Deterministic, repeatable pipelines using DVC Parameter-driven experimentation Tools used: Python DVC Git & GitHub Cookiecutter Data Science","title":"Overview"},{"location":"project_overview/#project-overview","text":"The project began as a single Jupyter notebook used for experimentation. Once the modeling approach was validated, the notebook was refactored into a production-style pipeline composed of independent Python modules.","title":"Project Overview"},{"location":"project_overview/#key-ideas-demonstrated","text":"Separation of concerns Script-based execution instead of notebooks Deterministic, repeatable pipelines using DVC Parameter-driven experimentation","title":"Key ideas demonstrated:"},{"location":"project_overview/#tools-used","text":"Python DVC Git & GitHub Cookiecutter Data Science","title":"Tools used:"},{"location":"project_structure/","text":"Project Structure (CCDS) The repository follows the Cookiecutter Data Science layout to keep data, code, and artifacts cleanly separated. emotion_detection_MLOps_pipeline/ \u251c\u2500\u2500 data/ \u2502 \u251c\u2500\u2500 raw/ # Train/test splits \u2502 \u251c\u2500\u2500 interim/ # Cleaned & normalized text \u2502 \u2514\u2500\u2500 processed/ # Feature-engineered datasets \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 data/ \u2502 \u2502 \u251c\u2500\u2500 data_ingestion.py \u2502 \u2502 \u2514\u2500\u2500 data_preprocessing.py \u2502 \u251c\u2500\u2500 features/ \u2502 \u2502 \u2514\u2500\u2500 feature_engineering.py \u2502 \u2514\u2500\u2500 model/ \u2502 \u251c\u2500\u2500 model_building.py \u2502 \u2514\u2500\u2500 model_evaluation.py \u251c\u2500\u2500 models/ # Trained model artifacts \u251c\u2500\u2500 reports/ # Metrics and evaluation results \u251c\u2500\u2500 notebooks/ # Exploratory work \u251c\u2500\u2500 docs/ # Documentation \u251c\u2500\u2500 dvc.yaml \u251c\u2500\u2500 dvc.lock \u251c\u2500\u2500 params.yaml \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 README.md This structure makes it clear what is code, what is data, and what is output.","title":"Project Structure"},{"location":"reproduction/","text":"Reproducing the Project Clone the repository and run: git clone https://github.com/yachika-yashu/emotion_detection_MLOps_pipeline.git cd emotion_detection_MLOps_pipeline dvc pull dvc repro This fetches the correct data and artifacts and rebuilds the full ML pipeline end-to-end.","title":"Reproduction"},{"location":"reproduction/#reproducing-the-project","text":"Clone the repository and run: git clone https://github.com/yachika-yashu/emotion_detection_MLOps_pipeline.git cd emotion_detection_MLOps_pipeline dvc pull dvc repro This fetches the correct data and artifacts and rebuilds the full ML pipeline end-to-end.","title":"Reproducing the Project"},{"location":"why_mlops/","text":"Why This Project Matters Many ML projects stop at model training. This project focuses on engineering maturity: Reproducible experiments Traceable data and models Clear separation between experimentation and production code Pipeline automation The result is not just a trained model, but a reliable ML system.","title":"Why MLOps"},{"location":"why_mlops/#why-this-project-matters","text":"Many ML projects stop at model training. This project focuses on engineering maturity: Reproducible experiments Traceable data and models Clear separation between experimentation and production code Pipeline automation The result is not just a trained model, but a reliable ML system.","title":"Why This Project Matters"}]}